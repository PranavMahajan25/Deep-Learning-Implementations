{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "       \n",
    " #      if self.load_model:\n",
    " #            self.model.load_weights(\"./save_model/cartpole_dqn.h5\")\n",
    "              \n",
    "       \n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input) #update_input == s_t =~current_state\n",
    "        target_val = self.target_model.predict(update_target) #Q_sa == target_val, update_target = s_t1 =~ next_state\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    np.amax(target_val[i]))\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-27 21:04:26,488] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "('episode:', 0, '  score:', 14.0, '  memory length:', 15, '  epsilon:', 0.9851045463620021)\n",
      "('episode:', 1, '  score:', 19.0, '  memory length:', 35, '  epsilon:', 0.9655885070369844)\n",
      "('episode:', 2, '  score:', 9.0, '  memory length:', 45, '  epsilon:', 0.9559759577813409)\n",
      "('episode:', 3, '  score:', 18.0, '  memory length:', 64, '  epsilon:', 0.9379749638258457)\n",
      "('episode:', 4, '  score:', 20.0, '  memory length:', 85, '  epsilon:', 0.9184732224159486)\n",
      "('episode:', 5, '  score:', 11.0, '  memory length:', 97, '  epsilon:', 0.9075119613694457)\n",
      "('episode:', 6, '  score:', 10.0, '  memory length:', 108, '  epsilon:', 0.8975790935118436)\n",
      "('episode:', 7, '  score:', 34.0, '  memory length:', 143, '  epsilon:', 0.8666920568517111)\n",
      "('episode:', 8, '  score:', 15.0, '  memory length:', 159, '  epsilon:', 0.8529285032149548)\n",
      "('episode:', 9, '  score:', 31.0, '  memory length:', 191, '  epsilon:', 0.8260536436246144)\n",
      "('episode:', 10, '  score:', 12.0, '  memory length:', 204, '  epsilon:', 0.8153791427799216)\n",
      "('episode:', 11, '  score:', 14.0, '  memory length:', 219, '  epsilon:', 0.8032337005612527)\n",
      "('episode:', 12, '  score:', 22.0, '  memory length:', 242, '  epsilon:', 0.7849611281333895)\n",
      "('episode:', 13, '  score:', 30.0, '  memory length:', 273, '  epsilon:', 0.7609888362515699)\n",
      "('episode:', 14, '  score:', 13.0, '  memory length:', 287, '  epsilon:', 0.750403966288439)\n",
      "('episode:', 15, '  score:', 11.0, '  memory length:', 299, '  epsilon:', 0.7414484806367364)\n",
      "('episode:', 16, '  score:', 27.0, '  memory length:', 327, '  epsilon:', 0.7209657768279322)\n",
      "('episode:', 17, '  score:', 10.0, '  memory length:', 338, '  epsilon:', 0.7130746876787832)\n",
      "('episode:', 18, '  score:', 9.0, '  memory length:', 348, '  epsilon:', 0.7059759437435444)\n",
      "('episode:', 19, '  score:', 18.0, '  memory length:', 367, '  epsilon:', 0.6926824413362807)\n",
      "('episode:', 20, '  score:', 20.0, '  memory length:', 388, '  epsilon:', 0.6782806562448989)\n",
      "('episode:', 21, '  score:', 13.0, '  memory length:', 402, '  epsilon:', 0.6688462043806328)\n",
      "('episode:', 22, '  score:', 11.0, '  memory length:', 414, '  epsilon:', 0.660864046961939)\n",
      "('episode:', 23, '  score:', 9.0, '  memory length:', 424, '  epsilon:', 0.6542850662093624)\n",
      "('episode:', 24, '  score:', 9.0, '  memory length:', 434, '  epsilon:', 0.6477715799982753)\n",
      "('episode:', 25, '  score:', 13.0, '  memory length:', 448, '  epsilon:', 0.6387614899503485)\n",
      "('episode:', 26, '  score:', 9.0, '  memory length:', 458, '  epsilon:', 0.632402542800493)\n",
      "('episode:', 27, '  score:', 14.0, '  memory length:', 473, '  epsilon:', 0.6229826200436561)\n",
      "('episode:', 28, '  score:', 19.0, '  memory length:', 493, '  epsilon:', 0.6106406271491208)\n",
      "('episode:', 29, '  score:', 10.0, '  memory length:', 504, '  epsilon:', 0.6039570649304998)\n",
      "('episode:', 30, '  score:', 16.0, '  memory length:', 521, '  epsilon:', 0.5937715237303958)\n",
      "('episode:', 31, '  score:', 9.0, '  memory length:', 531, '  epsilon:', 0.5878604570836192)\n",
      "('episode:', 32, '  score:', 8.0, '  memory length:', 540, '  epsilon:', 0.5825908266400397)\n",
      "('episode:', 33, '  score:', 12.0, '  memory length:', 553, '  epsilon:', 0.5750624217730242)\n",
      "('episode:', 34, '  score:', 12.0, '  memory length:', 566, '  epsilon:', 0.5676313011014509)\n",
      "('episode:', 35, '  score:', 10.0, '  memory length:', 577, '  epsilon:', 0.561418483038787)\n",
      "('episode:', 36, '  score:', 8.0, '  memory length:', 586, '  epsilon:', 0.5563858806683429)\n",
      "('episode:', 37, '  score:', 11.0, '  memory length:', 598, '  epsilon:', 0.5497458494385241)\n",
      "('episode:', 38, '  score:', 11.0, '  memory length:', 610, '  epsilon:', 0.5431850617989273)\n",
      "('episode:', 39, '  score:', 9.0, '  memory length:', 620, '  epsilon:', 0.5377775894404436)\n",
      "('episode:', 40, '  score:', 10.0, '  memory length:', 631, '  epsilon:', 0.531891525167934)\n",
      "('episode:', 41, '  score:', 13.0, '  memory length:', 645, '  epsilon:', 0.5244932528672183)\n",
      "('episode:', 42, '  score:', 10.0, '  memory length:', 656, '  epsilon:', 0.5187525878460405)\n",
      "('episode:', 43, '  score:', 10.0, '  memory length:', 667, '  epsilon:', 0.5130747553488376)\n",
      "('episode:', 44, '  score:', 16.0, '  memory length:', 684, '  epsilon:', 0.5044219150017507)\n",
      "('episode:', 45, '  score:', 13.0, '  memory length:', 698, '  epsilon:', 0.4974057274803321)\n",
      "('episode:', 46, '  score:', 9.0, '  memory length:', 708, '  epsilon:', 0.49245399387890804)\n",
      "('episode:', 47, '  score:', 11.0, '  memory length:', 720, '  epsilon:', 0.4865769398194536)\n",
      "('episode:', 48, '  score:', 11.0, '  memory length:', 732, '  epsilon:', 0.48077002381319206)\n",
      "('episode:', 49, '  score:', 15.0, '  memory length:', 748, '  epsilon:', 0.4731351274767304)\n",
      "('episode:', 50, '  score:', 13.0, '  memory length:', 762, '  epsilon:', 0.4665541192401326)\n",
      "('episode:', 51, '  score:', 10.0, '  memory length:', 773, '  epsilon:', 0.46144760757736725)\n",
      "('episode:', 52, '  score:', 10.0, '  memory length:', 784, '  epsilon:', 0.4563969875256424)\n",
      "('episode:', 53, '  score:', 10.0, '  memory length:', 795, '  epsilon:', 0.4514016473420717)\n",
      "('episode:', 54, '  score:', 10.0, '  memory length:', 806, '  epsilon:', 0.4464609819793951)\n",
      "('episode:', 55, '  score:', 7.0, '  memory length:', 814, '  epsilon:', 0.4429017700604677)\n",
      "('episode:', 56, '  score:', 10.0, '  memory length:', 825, '  epsilon:', 0.438054137254317)\n",
      "('episode:', 57, '  score:', 11.0, '  memory length:', 837, '  epsilon:', 0.43282630302490405)\n",
      "('episode:', 58, '  score:', 13.0, '  memory length:', 851, '  epsilon:', 0.4268059648597502)\n",
      "('episode:', 59, '  score:', 15.0, '  memory length:', 867, '  epsilon:', 0.42002804790136294)\n",
      "('episode:', 60, '  score:', 16.0, '  memory length:', 884, '  epsilon:', 0.4129444102795546)\n",
      "('episode:', 61, '  score:', 14.0, '  memory length:', 899, '  epsilon:', 0.4067934159611651)\n",
      "('episode:', 62, '  score:', 12.0, '  memory length:', 912, '  epsilon:', 0.4015367153875324)\n",
      "('episode:', 63, '  score:', 10.0, '  memory length:', 923, '  epsilon:', 0.3971418299163796)\n",
      "('episode:', 64, '  score:', 9.0, '  memory length:', 933, '  epsilon:', 0.3931882354258422)\n",
      "('episode:', 65, '  score:', 8.0, '  memory length:', 942, '  epsilon:', 0.3896636631051653)\n",
      "('episode:', 66, '  score:', 10.0, '  memory length:', 953, '  epsilon:', 0.3853987301463841)\n",
      "('episode:', 67, '  score:', 9.0, '  memory length:', 963, '  epsilon:', 0.3815620396207659)\n",
      "('episode:', 68, '  score:', 10.0, '  memory length:', 974, '  epsilon:', 0.3773857802651195)\n",
      "('episode:', 69, '  score:', 10.0, '  memory length:', 985, '  epsilon:', 0.37325523075582717)\n",
      "('episode:', 70, '  score:', 16.0, '  memory length:', 1002, '  epsilon:', 0.3669604016168464)\n",
      "('episode:', 71, '  score:', 12.0, '  memory length:', 1015, '  epsilon:', 0.3622184346183837)\n",
      "('episode:', 72, '  score:', 14.0, '  memory length:', 1030, '  epsilon:', 0.3568230267186975)\n",
      "('episode:', 73, '  score:', 12.0, '  memory length:', 1043, '  epsilon:', 0.3522120577707228)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('episode:', 74, '  score:', 8.0, '  memory length:', 1052, '  epsilon:', 0.3490547993433876)\n",
      "('episode:', 75, '  score:', 13.0, '  memory length:', 1066, '  epsilon:', 0.3441996694320795)\n",
      "('episode:', 76, '  score:', 8.0, '  memory length:', 1075, '  epsilon:', 0.34111423472584396)\n",
      "('episode:', 77, '  score:', 10.0, '  memory length:', 1086, '  epsilon:', 0.33738068325533116)\n",
      "('episode:', 78, '  score:', 12.0, '  memory length:', 1099, '  epsilon:', 0.3330209538162239)\n",
      "('episode:', 79, '  score:', 8.0, '  memory length:', 1108, '  epsilon:', 0.3300357260543739)\n",
      "('episode:', 80, '  score:', 8.0, '  memory length:', 1117, '  epsilon:', 0.32707725812456445)\n",
      "('episode:', 81, '  score:', 10.0, '  memory length:', 1128, '  epsilon:', 0.323497343674428)\n",
      "('episode:', 82, '  score:', 13.0, '  memory length:', 1142, '  epsilon:', 0.3189977016914014)\n",
      "('episode:', 83, '  score:', 12.0, '  memory length:', 1155, '  epsilon:', 0.3148755223844758)\n",
      "('episode:', 84, '  score:', 24.0, '  memory length:', 1180, '  epsilon:', 0.30709737673437937)\n",
      "('episode:', 85, '  score:', 9.0, '  memory length:', 1190, '  epsilon:', 0.30404018556171647)\n",
      "('episode:', 86, '  score:', 10.0, '  memory length:', 1201, '  epsilon:', 0.3007124156643058)\n",
      "('episode:', 87, '  score:', 15.0, '  memory length:', 1217, '  epsilon:', 0.29593693465058934)\n",
      "('episode:', 88, '  score:', 10.0, '  memory length:', 1228, '  epsilon:', 0.2926978561687671)\n",
      "('episode:', 89, '  score:', 14.0, '  memory length:', 1243, '  epsilon:', 0.28833798882226386)\n",
      "('episode:', 90, '  score:', 13.0, '  memory length:', 1257, '  epsilon:', 0.284327391068757)\n",
      "('episode:', 91, '  score:', 11.0, '  memory length:', 1269, '  epsilon:', 0.28093416557223355)\n",
      "('episode:', 92, '  score:', 9.0, '  memory length:', 1279, '  epsilon:', 0.2781374323007875)\n",
      "('episode:', 93, '  score:', 11.0, '  memory length:', 1291, '  epsilon:', 0.27481807913093287)\n",
      "('episode:', 94, '  score:', 11.0, '  memory length:', 1303, '  epsilon:', 0.27153833985042447)\n",
      "('episode:', 95, '  score:', 12.0, '  memory length:', 1316, '  epsilon:', 0.268029443956713)\n",
      "('episode:', 96, '  score:', 15.0, '  memory length:', 1332, '  epsilon:', 0.2637729867768368)\n",
      "('episode:', 97, '  score:', 48.0, '  memory length:', 1381, '  epsilon:', 0.25115350309155987)\n",
      "('episode:', 98, '  score:', 29.0, '  memory length:', 1411, '  epsilon:', 0.24372713693665488)\n",
      "('episode:', 99, '  score:', 49.0, '  memory length:', 1461, '  epsilon:', 0.23183462439849456)\n",
      "('episode:', 100, '  score:', 23.0, '  memory length:', 1485, '  epsilon:', 0.22633411298963688)\n",
      "('episode:', 101, '  score:', 48.0, '  memory length:', 1534, '  epsilon:', 0.2155057879166419)\n",
      "('episode:', 102, '  score:', 48.0, '  memory length:', 1583, '  epsilon:', 0.20519551388923485)\n",
      "('episode:', 103, '  score:', 35.0, '  memory length:', 1619, '  epsilon:', 0.1979362954770861)\n",
      "('episode:', 104, '  score:', 43.0, '  memory length:', 1663, '  epsilon:', 0.18941175139979763)\n",
      "('episode:', 105, '  score:', 53.0, '  memory length:', 1717, '  epsilon:', 0.17944992617887773)\n",
      "('episode:', 106, '  score:', 40.0, '  memory length:', 1758, '  epsilon:', 0.1722377332480149)\n",
      "('episode:', 107, '  score:', 41.0, '  memory length:', 1800, '  epsilon:', 0.1651500869836984)\n",
      "('episode:', 108, '  score:', 106.0, '  memory length:', 1907, '  epsilon:', 0.14838364901724713)\n",
      "('episode:', 109, '  score:', 129.0, '  memory length:', 2000, '  epsilon:', 0.13028652967534501)\n",
      "('episode:', 110, '  score:', 59.0, '  memory length:', 2000, '  epsilon:', 0.12269554947448735)\n",
      "('episode:', 111, '  score:', 126.0, '  memory length:', 2000, '  epsilon:', 0.10805523559661097)\n",
      "('episode:', 112, '  score:', 45.0, '  memory length:', 2000, '  epsilon:', 0.10319490913541582)\n",
      "('episode:', 113, '  score:', 77.0, '  memory length:', 2000, '  epsilon:', 0.09544789492655645)\n",
      "('episode:', 114, '  score:', 80.0, '  memory length:', 2000, '  epsilon:', 0.08801787938827345)\n",
      "('episode:', 115, '  score:', 59.0, '  memory length:', 2000, '  epsilon:', 0.08288962874392225)\n",
      "('episode:', 116, '  score:', 83.0, '  memory length:', 2000, '  epsilon:', 0.07620811252982171)\n",
      "('episode:', 117, '  score:', 245.0, '  memory length:', 2000, '  epsilon:', 0.059581483043601974)\n",
      "('episode:', 118, '  score:', 68.0, '  memory length:', 2000, '  epsilon:', 0.055607068006314264)\n",
      "('episode:', 119, '  score:', 134.0, '  memory length:', 2000, '  epsilon:', 0.04858149856964864)\n",
      "('episode:', 120, '  score:', 305.0, '  memory length:', 2000, '  epsilon:', 0.035769288728191656)\n",
      "('episode:', 121, '  score:', 348.0, '  memory length:', 2000, '  epsilon:', 0.025227005109553523)\n",
      "('episode:', 122, '  score:', 489.0, '  memory length:', 2000, '  epsilon:', 0.015450940705938459)\n",
      "('episode:', 123, '  score:', 496.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 124, '  score:', 119.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 125, '  score:', 86.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 126, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 127, '  score:', 491.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 128, '  score:', 352.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 129, '  score:', 73.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 130, '  score:', 157.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 131, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 132, '  score:', 72.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 133, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 134, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 135, '  score:', 322.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 136, '  score:', 172.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 137, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 138, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 139, '  score:', 213.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 140, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 141, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 142, '  score:', 374.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 143, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 144, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 145, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 146, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 147, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 148, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 149, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 150, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 151, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n",
      "('episode:', 152, '  score:', 500.0, '  memory length:', 2000, '  epsilon:', 0.009998671593271896)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/lab1/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, maximum length of episode is 500\n",
    "    env = gym.make('CartPole-v1')\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "    \n",
    "    EPISODES = 300\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            env.render()\n",
    "\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "            reward = reward if not done or score == 499 else -100\n",
    "\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # every time step do the training\n",
    "            agent.train_model()\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                agent.update_target_model()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                score = score if score == 500 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "               # pylab.savefig(\"./save_graph/cartpole_dqn.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                    sys.exit()\n",
    "\n",
    "        # save the model\n",
    " #       if e % 50 == 0:\n",
    " #           agent.model.save_weights(\"./save_model/cartpole_dqn.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-27 18:03:42,276] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01314833  0.00989805  0.0278539   0.03632513]\n",
      "(4,)\n",
      "[[ 0.01314833  0.00989805  0.0278539   0.03632513]]\n",
      "(1, 4)\n",
      "4\n",
      "Box(4,)\n",
      "(4,)\n",
      "4\n",
      "[ 0.0133463   0.20460973  0.0285804  -0.2474411 ]\n",
      "[[ 0.01314833  0.00989805  0.0278539   0.03632513]]\n",
      "deque([(array([[ 0.01314833,  0.00989805,  0.0278539 ,  0.03632513]]), 1, 1.0, array([[ 0.01314833,  0.00989805,  0.0278539 ,  0.03632513]]), False)], maxlen=5)\n"
     ]
    }
   ],
   "source": [
    "#feelcomfortable\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "env = gym.make('CartPole-v1')\n",
    "state = env.reset()\n",
    "print state\n",
    "print state.shape\n",
    "state = np.reshape(state, [1, state.size])\n",
    "print state\n",
    "print state.shape\n",
    "print state.size\n",
    "print env.observation_space\n",
    "print env.observation_space.shape\n",
    "print env.observation_space.shape[0]\n",
    "action = random.randrange(env.action_space.n)\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print next_state\n",
    "next_state = np.reshape(state, [1, next_state.size])\n",
    "print next_state\n",
    "\n",
    "memory = deque(maxlen=5)\n",
    "memory.append((state, action, reward, next_state, done))\n",
    "print memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
