{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[2017-12-29 11:20:35,598] Making new env: MsPacman-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import sys\n",
    "import pylab\n",
    "import random\n",
    "import os\n",
    "import operator\n",
    "from collections import deque\n",
    "\n",
    "from skimage import io, color, transform\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense,Convolution2D,Flatten,Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "GAME_TYPE = 'MsPacman-v0'\n",
    "env =gym.make(\"MsPacman-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "# Hyperparameters\n",
    "#####\n",
    "\n",
    "#environment parameters\n",
    "NUM_EPISODES = 80000000\n",
    "MAX_TIMESTEPS = 10000\n",
    "#FRAME_SKIP = \n",
    "PHI_LENGTH = 4\n",
    "\n",
    "#agent parameters\n",
    "#NAIVE_RANDOM = \n",
    "EPSILON = 1\n",
    "EXPERIENCE_REPLAY_CAPACITY = 2000\n",
    "MINIBATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "ACTION_SIZE = env.action_space.n\n",
    "EXPLORE = 3000000\n",
    "print ACTION_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PREPROCESS_IMAGE_DIM = 84\n",
    "\n",
    "def preprocess_observation(observation):\n",
    "    \"\"\"\n",
    "    preprocesses a given observation following the steps described in the paper\n",
    "    \"\"\"\n",
    "    grayscale_observation = color.rgb2gray(observation)\n",
    "    resized_observation = transform.resize(grayscale_observation, (PREPROCESS_IMAGE_DIM, PREPROCESS_IMAGE_DIM)).astype('float32')\n",
    "    return resized_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets begin\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, epsilon , experience_replay_capacity , minibatch_size , learning_rate ,action_size, img_dim , explore):\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = minibatch_size\n",
    "        self.train_start = 1000\n",
    "        self.explore = explore\n",
    "        self.img_rows = img_dim\n",
    "        self.img_cols = img_dim\n",
    "        self.img_channels = 4 #phi_length  #coz we feed in 4 stacked b&w imgs instead of 1 rbg img\n",
    "        \n",
    "         # create replay memory using deque\n",
    "        self.D = deque(maxlen=experience_replay_capacity)\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "        \n",
    "    def build_model(self) :\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(32, 8, 8, subsample=(4, 4), border_mode='same',input_shape=(self.img_rows,self.img_cols,self.img_channels)))  #84*84*4\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(64, 4, 4, subsample=(2, 2), border_mode='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(9))\n",
    "                                \n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        #print(\"finish building the model\")\n",
    "        return model\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        \n",
    "    def append_experience_replay_example(self,s_t,a_t,r_t,s_t1,done):\n",
    "        \"\"\"\n",
    "        Add an experience replay example to our agent's replay memory. If\n",
    "        memory is full, overwrite previous examples, starting with the oldest\n",
    "        \"\"\"\n",
    "        self.D.append((s_t, a_t, r_t, s_t1, done))\n",
    "       \n",
    "        if self.epsilon > self.epsilon_min :\n",
    "            self.epsilon -= (self.epsilon - self.epsilon_min) /self.explore\n",
    "        \n",
    "    def preprocess_observation(self, observation, prediction=False):\n",
    "        \"\"\"\n",
    "        Helper function for preprocessing an observation for consumption by our\n",
    "        deep learning network\n",
    "        \"\"\"\n",
    "        grayscale_observation = color.rgb2gray(observation)\n",
    "        resized_observation = transform.resize(grayscale_observation, (1, self.processed_image_dim, self.processed_image_dim)).astype('float32')\n",
    "        if prediction:\n",
    "            resized_observation = np.expand_dims(resized_observation, 0)\n",
    "        return resized_observation\n",
    "        \n",
    "    def take_action(self, s_t):\n",
    "        \"\"\"\n",
    "        Given an observation, the model attempts to take an action\n",
    "        according to its q-function approximation\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(s_t)\n",
    "            return np.argmax(q_value[0])    \n",
    "        \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Allow the model to collect examples from its experience replay memory\n",
    "        and learn from them\n",
    "        \"\"\"\n",
    "        if len(self.D) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.D))\n",
    "        mini_batch = random.sample(self.D, batch_size)\n",
    "        \n",
    "        update_input = np.zeros((batch_size, 84, 84, 4)) #batch_size,84,84,4\n",
    "        update_target = np.zeros((batch_size, 84, 84, 4))\n",
    "        action, reward, done = [], [], []\n",
    "        \n",
    "        #Now we do the experience replay\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0] # array of s_t that were copied into mini_batch \n",
    "            action.append(mini_batch[i][1])    # array of action indices = a_t of actions taken\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3] # array of s_t1 that were copied into mini_batch \n",
    "            done.append(mini_batch[i][4])\n",
    "            \n",
    "        target = self.model.predict(update_input)\n",
    "        Q_sa = self.model.predict(update_target) #target_val\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (np.amax(Q_sa[i]))\n",
    "                \n",
    "         # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_simulation():\n",
    "    \"\"\"\n",
    "    Entry-point for running Ms. Pac-man simulation\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    #print game parameters\n",
    "    print (\"~~~Environment Parameters~~~\")\n",
    "    print (\"Num episodes: %s\" % NUM_EPISODES)\n",
    " #   print (\"Max timesteps: %s\" % MAX_TIMESTEPS)\n",
    "    print (\"Action space: %s\" % env.action_space)\n",
    "    print()\n",
    "    print (\"~~~Agent Parameters~~~\")\n",
    " #   print (\"Naive Random: %s\" % NAIVE_RANDOM)\n",
    "    print (\"Epsilon: %s\" % EPSILON)\n",
    "    print (\"Experience Replay Capacity: %s\" % EXPERIENCE_REPLAY_CAPACITY)\n",
    "    print (\"Minibatch Size: %s\" % MINIBATCH_SIZE)\n",
    "    print (\"Learning Rate: %s\" % LEARNING_RATE)\n",
    "\n",
    "    #initialize agent\n",
    "    agent = Agent(epsilon=EPSILON,\n",
    "                experience_replay_capacity=EXPERIENCE_REPLAY_CAPACITY,\n",
    "                minibatch_size=MINIBATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE, action_size =ACTION_SIZE, img_dim =PREPROCESS_IMAGE_DIM ,explore =EXPLORE)\n",
    "    \n",
    "    scores, episodes = [], [] #note the s\n",
    "\n",
    "    #initialize auxiliary data structures\n",
    "    state_list = [] \n",
    "    tot_frames = 0\n",
    "    \n",
    "    \n",
    "    for i_episode in range(NUM_EPISODES):\n",
    "        print (\"Episode: %s\" % i_episode)\n",
    "        env.render() \n",
    "        done = False\n",
    "        score = 0\n",
    "        x_t = env.reset()\n",
    "        x_t = preprocess_observation(x_t)   \n",
    "        s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "        s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*84*84*4\n",
    "\n",
    "        \n",
    "        t=0\n",
    "        while not done:\n",
    "           # get action for the current state and go one step in environment\n",
    "            \n",
    "            a_t = agent.take_action(s_t)      \n",
    "            x_t1 , r_t, done, info = env.step(a_t)\n",
    "            x_t1=preprocess_observation(x_t1)\n",
    "            x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1],1) #1x84x84x1\n",
    "            s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "            agent.append_experience_replay_example(s_t,a_t,r_t,s_t1,done)\n",
    "            agent.learn()\n",
    "            score += r_t\n",
    "            s_t=s_t1\n",
    "            t+=1\n",
    "            if t>MAX_TIMESTEPS:\n",
    "                print \"yolo beeyatch\"\n",
    "                done = True\n",
    "        if done:\n",
    "            # every episode update the target model to be same with mode\n",
    "            agent.update_target_model() \n",
    "            scores.append(score)\n",
    "            episodes.append(i_episode)\n",
    "            print \"  score:%s \" %(score,), \"  epsilon: %s\" %(agent.epsilon,)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-29 11:21:02,254] Making new env: MsPacman-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~Environment Parameters~~~\n",
      "Num episodes: 80000000\n",
      "Action space: Discrete(9)\n",
      "()\n",
      "~~~Agent Parameters~~~\n",
      "Epsilon: 1\n",
      "Experience Replay Capacity: 2000\n",
      "Minibatch Size: 128\n",
      "Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/lab1/lib/python2.7/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), padding=\"same\", strides=(4, 4), input_shape=(84, 84, 4...)`\n",
      "/home/pranav/lab1/lib/python2.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (4, 4), padding=\"same\", strides=(2, 2))`\n",
      "/home/pranav/lab1/lib/python2.7/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/lab1/local/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  score:190.0    epsilon: 0.9997855232\n",
      "Episode: 1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env =gym.make(\"MsPacman-v0\")\n",
    "    run_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
