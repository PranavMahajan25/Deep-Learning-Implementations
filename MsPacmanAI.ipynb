{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[2017-12-27 20:56:39,028] Making new env: MsPacman-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import sys\n",
    "import pylab\n",
    "import random\n",
    "import os\n",
    "import operator\n",
    "from collections import deque\n",
    "\n",
    "from skimage import io, color, transform\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense,Convolution2D,Flatten,Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "GAME_TYPE = 'MsPacman-v0'\n",
    "env =gym.make(\"MsPacman-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "# Hyperparameters\n",
    "#####\n",
    "\n",
    "#environment parameters\n",
    "NUM_EPISODES = 80000000\n",
    "#MAX_TIMESTEPS = \n",
    "#FRAME_SKIP = \n",
    "PHI_LENGTH = 4\n",
    "\n",
    "#agent parameters\n",
    "#NAIVE_RANDOM = \n",
    "EPSILON = 1\n",
    "EXPERIENCE_REPLAY_CAPACITY = 2000\n",
    "MINIBATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.001\n",
    "ACTION_SIZE = env.action_space.n\n",
    "EXPLORE = 3000000\n",
    "print ACTION_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PREPROCESS_IMAGE_DIM = 84\n",
    "\n",
    "def preprocess_observation(observation):\n",
    "    \"\"\"\n",
    "    preprocesses a given observation following the steps described in the paper\n",
    "    \"\"\"\n",
    "    grayscale_observation = color.rgb2gray(observation)\n",
    "    resized_observation = transform.resize(grayscale_observation, (PREPROCESS_IMAGE_DIM, PREPROCESS_IMAGE_DIM)).astype('float32')\n",
    "    return resized_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State.shape = (210, 160, 3)\n",
      "State_size = 100800\n",
      "100800\n",
      "State.shape now = (1, 100800)\n",
      "State_size now = 100800\n",
      "env.observation_space.shape = (210, 160, 3)\n",
      "next_State.shape = (210, 160, 3)\n",
      "next_State.shape now = (1, 100800)\n",
      "x_t.shape = (84, 84)\n",
      "s_t.shape = (84, 84, 4)\n",
      "s_t.shape now = (1, 84, 84, 4)\n",
      "(210, 160, 3)\n",
      "x_t1.shape = (84, 84)\n",
      "x_t1.shape now = (1, 84, 84, 1)\n",
      "s_t1.shape = (1, 84, 84, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/lab1/local/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "#ignore this cell please\n",
    "state =env.reset()\n",
    "#print state\n",
    "print \"State.shape = %s\" % (state.shape,)\n",
    "print \"State_size = %s\" % (state.size,)\n",
    "print 210*160*3\n",
    "#laying out all 100800 features in 1 row, for fun\n",
    "state = np.reshape(state, [1, state.size])\n",
    "#print state\n",
    "print \"State.shape now = %s\" % (state.shape,)\n",
    "print \"State_size now = %s\" % (state.size,)\n",
    "#print env.observation_space\n",
    "print \"env.observation_space.shape = %s\" %(env.observation_space.shape,)\n",
    "\n",
    "action = random.randrange(env.action_space.n)\n",
    "next_state, reward, done, info = env.step(action)\n",
    "#print next_state\n",
    "print \"next_State.shape = %s\" % (next_state.shape,)\n",
    "next_state = np.reshape(state, [1, next_state.size])\n",
    "#print next_state\n",
    "print \"next_State.shape now = %s\" % (next_state.shape,)\n",
    "\n",
    "#slight change of notations\n",
    "#ok let next_state be called x_t1 and current state be x_t\n",
    "# let reward be r_t and action a_t, 9=env.action_space.n= possible actions\n",
    "x_t = env.reset()\n",
    "x_t = preprocess_observation(x_t)\n",
    "print \"x_t.shape = %s\"% (x_t.shape,)\n",
    "\n",
    "s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "print \"s_t.shape = %s\"% (s_t.shape,)\n",
    "s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*84*84*4\n",
    "print \"s_t.shape now = %s\"% (s_t.shape,)\n",
    "\n",
    "#above stuff - one time per episode at the start eventualy set s_t = s_t1 and so on \n",
    "\n",
    "a_t = random.randrange(env.action_space.n)\n",
    "x_t1 , r_t, done, info = env.step(a_t)\n",
    "print x_t1.shape\n",
    "\n",
    "x_t1=preprocess_observation(x_t1)\n",
    "print \"x_t1.shape = %s\"% (x_t1.shape,)\n",
    "x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1],1) #1x84x84x1\n",
    "print \"x_t1.shape now = %s\"% (x_t1.shape,)\n",
    "s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "print \"s_t1.shape = %s\"% (s_t1.shape,)\n",
    "#now s_t =s_t1 and then loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([(array([[ 0.39959559,  0.39959559,  0.39959559, ...,  0.39959559,\n",
      "         0.39959559,  0.39959559],\n",
      "       [ 0.53279412,  0.53279412,  0.11700706, ...,  0.11700706,\n",
      "         0.53279412,  0.53279412],\n",
      "       [ 0.53279412,  0.53279412,  0.11700706, ...,  0.11700706,\n",
      "         0.53279412,  0.53279412],\n",
      "       ..., \n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ]], dtype=float32), 4, 0.0, array([[[[ 0.39959559],\n",
      "         [ 0.39959559],\n",
      "         [ 0.39959559],\n",
      "         ..., \n",
      "         [ 0.39959559],\n",
      "         [ 0.39959559],\n",
      "         [ 0.39959559]],\n",
      "\n",
      "        [[ 0.53279412],\n",
      "         [ 0.53279412],\n",
      "         [ 0.11700706],\n",
      "         ..., \n",
      "         [ 0.11700706],\n",
      "         [ 0.53279412],\n",
      "         [ 0.53279412]],\n",
      "\n",
      "        [[ 0.53279412],\n",
      "         [ 0.53279412],\n",
      "         [ 0.11700706],\n",
      "         ..., \n",
      "         [ 0.11700706],\n",
      "         [ 0.53279412],\n",
      "         [ 0.53279412]],\n",
      "\n",
      "        ..., \n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         ..., \n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         ..., \n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         ..., \n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]]]], dtype=float32), False), (array([[[[ 0.39959559,  0.39959559,  0.39959559,  0.39959559],\n",
      "         [ 0.39959559,  0.39959559,  0.39959559,  0.39959559],\n",
      "         [ 0.39959559,  0.39959559,  0.39959559,  0.39959559],\n",
      "         ..., \n",
      "         [ 0.39959559,  0.39959559,  0.39959559,  0.39959559],\n",
      "         [ 0.39959559,  0.39959559,  0.39959559,  0.39959559],\n",
      "         [ 0.39959559,  0.39959559,  0.39959559,  0.39959559]],\n",
      "\n",
      "        [[ 0.53279412,  0.53279412,  0.53279412,  0.53279412],\n",
      "         [ 0.53279412,  0.53279412,  0.53279412,  0.53279412],\n",
      "         [ 0.11700706,  0.11700706,  0.11700706,  0.11700706],\n",
      "         ..., \n",
      "         [ 0.11700706,  0.11700706,  0.11700706,  0.11700706],\n",
      "         [ 0.53279412,  0.53279412,  0.53279412,  0.53279412],\n",
      "         [ 0.53279412,  0.53279412,  0.53279412,  0.53279412]],\n",
      "\n",
      "        [[ 0.53279412,  0.53279412,  0.53279412,  0.53279412],\n",
      "         [ 0.53279412,  0.53279412,  0.53279412,  0.53279412],\n",
      "         [ 0.11700706,  0.11700706,  0.11700706,  0.11700706],\n",
      "         ..., \n",
      "         [ 0.11700706,  0.11700706,  0.11700706,  0.11700706],\n",
      "         [ 0.53279412,  0.53279412,  0.53279412,  0.53279412],\n",
      "         [ 0.53279412,  0.53279412,  0.53279412,  0.53279412]],\n",
      "\n",
      "        ..., \n",
      "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         ..., \n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
      "\n",
      "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         ..., \n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
      "\n",
      "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         ..., \n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ]]]], dtype=float32), 4, 0.0, array([[[[ 0.39959559,  0.39959559,  0.39959559,  0.39959559],\n",
      "         [ 0.39959559,  0.39959559,  0.39959559,  0.39959559],\n",
      "         [ 0.39959559,  0.39959559,  0.39959559,  0.39959559],\n",
      "         ..., \n",
      "         [ 0.39959559,  0.39959559,  0.39959559,  0.39959559],\n",
      "         [ 0.39959559,  0.39959559,  0.39959559,  0.39959559],\n",
      "         [ 0.39959559,  0.39959559,  0.39959559,  0.39959559]],\n",
      "\n",
      "        [[ 0.53279412,  0.53279412,  0.53279412,  0.53279412],\n",
      "         [ 0.53279412,  0.53279412,  0.53279412,  0.53279412],\n",
      "         [ 0.11700706,  0.11700706,  0.11700706,  0.11700706],\n",
      "         ..., \n",
      "         [ 0.11700706,  0.11700706,  0.11700706,  0.11700706],\n",
      "         [ 0.53279412,  0.53279412,  0.53279412,  0.53279412],\n",
      "         [ 0.53279412,  0.53279412,  0.53279412,  0.53279412]],\n",
      "\n",
      "        [[ 0.53279412,  0.53279412,  0.53279412,  0.53279412],\n",
      "         [ 0.53279412,  0.53279412,  0.53279412,  0.53279412],\n",
      "         [ 0.11700706,  0.11700706,  0.11700706,  0.11700706],\n",
      "         ..., \n",
      "         [ 0.11700706,  0.11700706,  0.11700706,  0.11700706],\n",
      "         [ 0.53279412,  0.53279412,  0.53279412,  0.53279412],\n",
      "         [ 0.53279412,  0.53279412,  0.53279412,  0.53279412]],\n",
      "\n",
      "        ..., \n",
      "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         ..., \n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
      "\n",
      "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         ..., \n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
      "\n",
      "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         ..., \n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ,  0.        ,  0.        ]]]], dtype=float32), False)], maxlen=5)\n"
     ]
    }
   ],
   "source": [
    "#visualising deque\n",
    "D = deque(maxlen=5)\n",
    "D.append((x_t, a_t, r_t, x_t1, done))\n",
    "D.append((s_t, a_t, r_t, s_t1, done))\n",
    "print D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets begin\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, epsilon , experience_replay_capacity , minibatch_size , learning_rate ,action_size, img_dim , explore):\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = minibatch_size\n",
    "        self.train_start = 1000\n",
    "        self.explore = explore\n",
    "        self.img_rows = img_dim\n",
    "        self.img_cols = img_dim\n",
    "        self.img_channels = 4 #phi_length  #coz we feed in 4 stacked b&w imgs instead of 1 rbg img\n",
    "        \n",
    "         # create replay memory using deque\n",
    "        self.D = deque(maxlen=experience_replay_capacity)\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "        \n",
    "    def build_model(self) :\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(32, 8, 8, subsample=(4, 4), border_mode='same',input_shape=(self.img_rows,self.img_cols,self.img_channels)))  #84*84*4\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(64, 4, 4, subsample=(2, 2), border_mode='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(9))\n",
    "                                \n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        #print(\"finish building the model\")\n",
    "        return model\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        \n",
    "    def append_experience_replay_example(self,s_t,a_t,r_t,s_t1,done):\n",
    "        \"\"\"\n",
    "        Add an experience replay example to our agent's replay memory. If\n",
    "        memory is full, overwrite previous examples, starting with the oldest\n",
    "        \"\"\"\n",
    "        D.append((s_t, a_t, r_t, s_t1, done))\n",
    "       \n",
    "        if self.epsilon > self.epsilon_min :\n",
    "            self.epsilon -= (self.epsilon - self.epsilon_min) /self.explore\n",
    "        \n",
    "    def preprocess_observation(self, observation, prediction=False):\n",
    "        \"\"\"\n",
    "        Helper function for preprocessing an observation for consumption by our\n",
    "        deep learning network\n",
    "        \"\"\"\n",
    "        grayscale_observation = color.rgb2gray(observation)\n",
    "        resized_observation = transform.resize(grayscale_observation, (1, self.processed_image_dim, self.processed_image_dim)).astype('float32')\n",
    "        if prediction:\n",
    "            resized_observation = np.expand_dims(resized_observation, 0)\n",
    "        return resized_observation\n",
    "        \n",
    "    def take_action(self, s_t):\n",
    "        \"\"\"\n",
    "        Given an observation, the model attempts to take an action\n",
    "        according to its q-function approximation\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(s_t)\n",
    "            return np.argmax(q_value[0])    \n",
    "        \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Allow the model to collect examples from its experience replay memory\n",
    "        and learn from them\n",
    "        \"\"\"\n",
    "        if len(self.D) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.D))\n",
    "        mini_batch = random.sample(self.D, batch_size)\n",
    "        \n",
    "        update_input = np.zeros((batch_size, s_t.shape[1], s_t.shape[2], s_t.shape[3])) #batch_size,84,84,4\n",
    "        print(inputs)\n",
    "        update_target = np.zeros((update_input.shape[0], self.action_size))\n",
    "        action, reward, done = [], [], []\n",
    "        \n",
    "        #Now we do the experience replay\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0] # array of s_t that were copied into mini_batch \n",
    "            action.append(mini_batch[i][1])    # array of action indices = a_t of actions taken\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3] # array of s_t1 that were copied into mini_batch \n",
    "            done.append(mini_batch[i][4])\n",
    "            \n",
    "        target = self.model.predict(update_input)\n",
    "        Q_sa = model.predict(update_target) #target_val\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (np.amax(Q_sa[i]))\n",
    "         # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_simulation():\n",
    "    \"\"\"\n",
    "    Entry-point for running Ms. Pac-man simulation\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    #print game parameters\n",
    "    print (\"~~~Environment Parameters~~~\")\n",
    "    print (\"Num episodes: %s\" % NUM_EPISODES)\n",
    " #   print (\"Max timesteps: %s\" % MAX_TIMESTEPS)\n",
    "    print (\"Action space: %s\" % env.action_space)\n",
    "    print()\n",
    "    print (\"~~~Agent Parameters~~~\")\n",
    " #   print (\"Naive Random: %s\" % NAIVE_RANDOM)\n",
    "    print (\"Epsilon: %s\" % EPSILON)\n",
    "    print (\"Experience Replay Capacity: %s\" % EXPERIENCE_REPLAY_CAPACITY)\n",
    "    print (\"Minibatch Size: %s\" % MINIBATCH_SIZE)\n",
    "    print (\"Learning Rate: %s\" % LEARNING_RATE)\n",
    "\n",
    "    #initialize agent\n",
    "    agent = Agent(epsilon=EPSILON,\n",
    "                experience_replay_capacity=EXPERIENCE_REPLAY_CAPACITY,\n",
    "                minibatch_size=MINIBATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE, action_size =ACTION_SIZE, img_dim =PREPROCESS_IMAGE_DIM ,explore =EXPLORE)\n",
    "    \n",
    "    scores, episodes = [], [] #note the s\n",
    "\n",
    "    #initialize auxiliary data structures\n",
    "    state_list = [] \n",
    "    tot_frames = 0\n",
    "\n",
    "    for i_episode in range(NUM_EPISODES):\n",
    "        print (\"Episode: %s\" % i_episode)\n",
    "         \n",
    "        done = False\n",
    "        score = 0\n",
    "        x_t = env.reset()\n",
    "        x_t = preprocess_observation(x_t)   \n",
    "        s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "        s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*84*84*4\n",
    "\n",
    "        \n",
    "        \n",
    "        while not done:\n",
    "            env.render()\n",
    "           # get action for the current state and go one step in environment\n",
    "            a_t = agent.take_action(s_t)      \n",
    "            x_t1 , r_t, done, info = env.step(a_t)\n",
    "            x_t1=preprocess_observation(x_t1)\n",
    "            x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1],1) #1x84x84x1\n",
    "            s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "            agent.append_experience_replay_example(s_t,a_t,r_t,s_t1,done)\n",
    "            agent.learn()\n",
    "            score += r_t\n",
    "            s_t=s_t1\n",
    "            \n",
    "        if done:\n",
    "            # every episode update the target model to be same with model\n",
    "            agent.update_target_model() \n",
    "            scores.append(score)\n",
    "            episodes.append(i_episode)\n",
    "            print( \"  score:\", score, \"  epsilon:\", agent.epsilon)\n",
    "            \n",
    "        while True:\n",
    "           \n",
    "            #ensure state list is populated\n",
    "            if tot_frames < PHI_LENGTH:\n",
    "                state_list.append(preprocess_observation(x_t))\n",
    "                tot_frames += 1\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "                #update state list with next observation\n",
    "                state_list.append(preprocess_observation(x_t))\n",
    "                state_list.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-27 21:08:28,542] Making new env: MsPacman-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~Environment Parameters~~~\n",
      "Num episodes: 80000000\n",
      "Action space: Discrete(9)\n",
      "()\n",
      "~~~Agent Parameters~~~\n",
      "Epsilon: 1\n",
      "Experience Replay Capacity: 2000\n",
      "Minibatch Size: 100\n",
      "Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/lab1/lib/python2.7/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), padding=\"same\", strides=(4, 4), input_shape=(84, 84, 4...)`\n",
      "/home/pranav/lab1/lib/python2.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (4, 4), padding=\"same\", strides=(2, 2))`\n",
      "/home/pranav/lab1/lib/python2.7/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "('  score:', 130.0, '  epsilon:', 0.999814557339431)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env =gym.make(\"MsPacman-v0\")\n",
    "    run_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
